{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning From Dr. Timnit Gebru\n",
    "author: Mead Gyawu\n",
    "date: '2023-04-19'\n",
    "description: \"Learning From Dr. Timnit Gebru\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Timnit Gebru\n",
    "\n",
    "Dr. Timnit Gebru, born an raised in Addis Ababa, Ethiopia, is a computer scientist with a PHD in computer Vision who dedicated her life to uncovering algorithmic bias against marginalized groups in artificial intelligence tools employed by major technology companies. Her passion for exposing bias was inspired by her own experience in school and with police. Having worked at major tech giants herself, Dr. Gebru understands the dangers that large language models could pose, which inspired her to write the paper \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\" However, she was then fired by Google after they saw what she had intened on publishing to be a threat. She now pursues her passion independently after she established Distributed Artificial Intelligence Research Institute (DAIR) and her work convinced the state of California to crack down on the racial biases present in major tech companies located there. Now, on Monday April 24th, Dr. Gebru has decided to hold a talk at middlebury about the use of artifical intelligence and how it intersects with the promotion of eugenics by some."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebru's Talk Regarding Computer Vision\n",
    "\n",
    "During her talk, Dr. Gebru made emphasized the various issues and dangers present in the rollout and use of of computer vision. Initiall, facial recogniton technology released by major tech companies were extremely accurate when guessing the race and gender of White men, but were almost random in identifying Black women. This is the result of datasets that consistented mainly of or focused purely on male eurocentric features.\n",
    "\n",
    "She highlighted the fact that because bias and intentionally flawed computer vision technology is often used by police, either to search for those with warrants or to find and arrest protestors, often times people have and will continue to be arrested by the police wrongfully. Given the fact that Black and hispanic populations are already disproportionately targeted, this will essentially become another tool in their oppression.\n",
    "\n",
    "This goes te same for tools made to identify those who are transgendered or gay. Facial recognition technology that would classify those as gay or transgendered would be dangerous tools in the hands of those who seek to persecute them. Essentially, computer vision has already become a tool that furthers the discrimination that marginalized communities face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question for Dr. Gebru\n",
    "\n",
    "##### The knowledge and impacts of highly flawed facial recognition tools is public knowledge. However, companies like Google and Amazon refuse to publically acknowledge the harm that their algorithims can cause. What incentive or reason would these businesses have for releasing largely flawed tools?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
