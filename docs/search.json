[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog woop"
  },
  {
    "objectID": "Implementing Perceptron/Implementing_perceptron.html",
    "href": "Implementing Perceptron/Implementing_perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom Perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-2, -2), (2, 2)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\"\"\"\nExperiment 1\n\nUsing 2d data like the data in the example, if the data is linearly separable then the perceptron algorithm converges to weight vector w\ndescribing a separating line (provided that the maximum number of iterations is large enough). Please show visualizations of the data, the \nseparating line, and the evolution of the accuracy over training. It’s also fine for you to use the loss instead of the accuracy if you’d \nprefer.\n\"\"\"\n\nP = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP.fit(X_,y)\nprint(\"History of Accuracy \\n\" + str(P.history))\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(P.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nHistory of Accuracy \n[0.75, 0.75, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.83, 0.86, 0.86, 0.87, 0.89, 0.9, 0.91, 0.91, 0.91, 0.91, 0.91, 0.91, 0.93, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\n\"\"\"\nExperiment 2\n\nFor 2d data, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w\n, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\nPlease show visualizations of the data, the line in the final iteration, and the evolution of the score over training.\n\"\"\"\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nP_2 = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP_2.fit(X_,y)\nprint(\"History of Accuracy \\n\" + str(P_2.history))\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(P_2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nHistory of Accuracy \n[0.59, 0.59, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.62, 0.62, 0.62, 0.58, 0.58, 0.58, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.58, 0.56, 0.56, 0.6, 0.6, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.58, 0.58, 0.58, 0.58, 0.58, 0.59, 0.59, 0.58, 0.58, 0.59, 0.59, 0.59, 0.58, 0.6, 0.6, 0.6, 0.6, 0.6, 0.62, 0.62, 0.62, 0.62, 0.62, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.63, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.65, 0.65, 0.65, 0.65, 0.65, 0.66, 0.66, 0.65, 0.66, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.69, 0.69, 0.68, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.67, 0.67, 0.67, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.7, 0.7, 0.7, 0.71, 0.71, 0.72, 0.71, 0.71, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.7, 0.7, 0.7, 0.7, 0.7, 0.71, 0.71, 0.71, 0.71, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.7, 0.7, 0.7, 0.7, 0.69, 0.69, 0.69, 0.69, 0.69, 0.68, 0.68, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.7, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.68, 0.68, 0.68, 0.68, 0.68, 0.69, 0.69, 0.69, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.67, 0.66, 0.65, 0.65, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.67, 0.67, 0.67, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.66, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.64, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.65, 0.65, 0.64, 0.64, 0.65, 0.64, 0.63, 0.63, 0.63, 0.63, 0.64, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.62, 0.62, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.66, 0.65, 0.66, 0.66, 0.66, 0.66, 0.66, 0.65, 0.64, 0.66, 0.64, 0.64, 0.64, 0.66, 0.65, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.63, 0.64, 0.63, 0.63, 0.63, 0.65, 0.65, 0.63, 0.65, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.63, 0.64, 0.63, 0.63, 0.64, 0.64, 0.65, 0.65, 0.64, 0.64, 0.63, 0.65, 0.65, 0.65, 0.65, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.63, 0.63, 0.64, 0.65, 0.65, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.62, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.62, 0.63, 0.62, 0.61, 0.62, 0.61, 0.61, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.63, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.63]\n\n\n\n\n\n\n\"\"\"\nExperiment 3\n\nThe perceptron algorithm is also able to work in more than 2 dimensions! Show an example of running your algorithm on data with at least 5 features. \nYou don’t need to visualize the data or the separating line, but you should still show the evolution of the score over the training period. \nInclude a comment on whether you believe that the data is linearly separable based on your observation of the score.\n\"\"\"\n\nn = 100\np_features = 10\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nP_3 = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP_3.fit(X_,y)\nprint(\"History of Accuracy \\n\" + str(P_3.history))\n\nHistory of Accuracy \n[0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.85, 0.86, 0.85, 0.86, 0.86, 0.85, 0.85, 0.85, 0.85, 0.86, 0.85, 0.85, 0.85, 0.85, 0.85, 0.84, 0.84, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.84, 0.83, 0.83, 0.83, 0.83, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.84, 0.83, 0.83, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Linear Regression\n\n\n\n\n\n\nMay 19, 2023\n\n\nMead Gyawu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Penguins\n\n\n\n\n\n\nMay 10, 2023\n\n\nMead Gyawu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe ran algorithms on HAR data to determine the best algorithm for HAR\n\n\n\n\n\n\nMay 8, 2023\n\n\nKaylynn Xia, Zayn Makdessi, and Mead Gyawu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning From Dr. Timnit Gebru\n\n\n\n\n\n\nApr 19, 2023\n\n\nMead Gyawu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Logistic Regression\n\n\n\n\n\n\nApr 17, 2023\n\n\nMead Gyawu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Perceptron Algorithm\n\n\n\n\n\n\nMar 9, 2023\n\n\nMead Gyawu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/LinearRegression/ImplementingLinearRegression.html",
    "href": "posts/LinearRegression/ImplementingLinearRegression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Experiments\nHere is a visualization of LinearRegression.fit_analytic(X,y) being run multiple times where the number of featurnes increases.\n\nn = 25\np= 1\n\ntrain_SCORES = []\nval_SCORES = []\n\nwhile p < n:\n    X_train, y_train, X_val, y_val = LR_data(n_train = n,p_features = p)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train, alpha = 0.001, max_epochs = 1e3)\n    train_SCORES.append(LR.score(X_train, y_train))\n    val_SCORES.append(LR.score(X_val, y_val))\n    p += 1\n\nplt.plot(train_SCORES, label = \"Training Scores\")\nplt.plot(val_SCORES, label = \"Validation Scores\")\nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\nlegend = plt.legend()\n\n\n\n\nWe see that even though the accuracy score of our Linear Regression model increases for our training data as the number of features increases. However, the accuracy score for our testing data takes a huge drop off the higher the number of features is. This essentially shows that as the number of features increase, we overfit to the training data and produce inaccurate results for testing or outside data.\nHere is a visualization of LinearRegression.fit_gradient(X,y) being run multiple times where the number of features increases.\n\nn = 25\np= 1\n\ntrain_SCORES = []\nval_SCORES = []\n\nwhile p < n:\n    X_train, y_train, X_val, y_val = LR_data(n_train = n,p_features = p)\n    LR = LinearRegression()\n    LR.fit_gradient(X_train, y_train, alpha = 0.001, max_epochs = 1e3)\n    train_SCORES.append(LR.score(X_train, y_train))\n    val_SCORES.append(LR.score(X_val, y_val))\n    p += 1\n\nplt.plot(train_SCORES, label = \"Training Scores\")\nplt.plot(val_SCORES, label = \"Validation Scores\")\nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\nlegend = plt.legend()\n\n\n\n\nHere, the scores of the validation data are consistently lower than that of the training data. This also shows how an increase in the number of features can ultimately lead to the model overfitting to the training data.\nHere is a visualization of a LASSO regularization model being run multiple times where the number of features increases.\n\nfrom sklearn.linear_model import Lasso\n\n\nn = 25\np= 1\n\ntrain_SCORES = []\nval_SCORES = []\n\nwhile p < n:\n    X_train, y_train, X_val, y_val = LR_data(n_train = n,p_features = p)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_SCORES.append(L.score(X_train, y_train))\n    val_SCORES.append(L.score(X_val, y_val))\n    p += 1\n\nplt.plot(train_SCORES, label = \"Training Scores\")\nplt.plot(val_SCORES, label = \"Validation Scores\")\nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\nlegend = plt.legend()\n\n\n\n\nMuch like the models before, the model seems to overfit to the training data as the number of features increases beyond a certain point."
  },
  {
    "objectID": "posts/LogisticRegression/Optimizing_logisitcregression.html",
    "href": "posts/LogisticRegression/Optimizing_logisitcregression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Seeing Logistic Regression in Action\n\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nPerforming Logistic Regression with gradient descent\nHere is an example of optimizing logisitc regression with gradient descent:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 1, max_epochs = 10000)\n\n# inspect the fitted value of w\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere is a graph of the change in loss calculated:\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere is a graph of the change in loss calculated when using stochastic gradient descent:\n\n# fit the model\n\nLR_sto = LogisticRegression()\nLR_sto.fit_stochastic(X, y, alpha = 0.01, max_epochs = 10000)\n\n\nfig = plt.plot(LR_sto.loss_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere is a graph comparing the evolution of loss for both gradient descent and stochastic gradient descent:\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100,  \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient descent\")\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient descent\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\n\nExperiments\n\nExperiment 1\nThe purpose of this experiment is to determine if there is a learning rate too high for the model to converge to a minimizer. As a result, I performed logistic regression on the data with varying values for the learning rate. I then plotted all of these models’ score history to determine if the models actually do converge to a minimizer.\n\n\"\"\"\nA case in which gradient descent does not converge to a minimizer because the learning rate \nalpha is too large.\n\"\"\"\nLR1 = 0\na = 0.1\n\nwhile a < 1000000:\n    LR1 = LogisticRegression()\n    LR1.fit(X, y, alpha = a, max_epochs=1000)\n\n    num_steps = len(LR1.loss_history)\n    plt.plot(np.arange(num_steps) + 1, LR1.score_history, label = \"alpha = \"+str(a))\n\n    a *= 100\n\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Score\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x2ac42757970>\n\n\n\n\n\n\n\nConclusion to Experiment 1\nThis graph shows that with a learning rate of 100,000, the model fails to converge.\n\n\nExperiment 2\nThe purpose of this experiment is to determine how the size of the batches in stocastic gradient descent affects how quickly the algorithm converges. In order to do this, we ran the algorithm with varying batch sizes and compared how long it takes for the algorithm to converge.\n\nLR = 0\nbatch = 100\n\nwhile batch < 400:\n    LR = LogisticRegression()\n    LR.fit_stochastic(X, y, \n                    max_epochs = 1000,  \n                    batch_size = batch, \n                    alpha = .05) \n\n    num_steps = len(LR.score_history)\n    plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"batch size: \"+str(batch))\n    batch += 100\n\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Score\")\nplt.legend()\n\n'\\nLR = LogisticRegression()\\nLR.fit_stochastic(X, y, \\n                  max_epochs = 1000,  \\n                  batch_size = 100, \\n                  alpha = .05) \\n\\nnum_steps = len(LR.score_history)\\nplt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"batch size: 100\")\\n\\nLR = LogisticRegression()\\nLR.fit_stochastic(X, y, \\n                  max_epochs = 1000,  \\n                  batch_size = 200, \\n                  alpha = .05) \\n\\nnum_steps = len(LR.score_history)\\nplt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"batch size: 200\")\\n\\nLR = LogisticRegression()\\nLR.fit_stochastic(X, y, \\n                  max_epochs = 1000,  \\n                  batch_size = 300, \\n                  alpha = .05) \\n\\nnum_steps = len(LR.score_history)\\nplt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"batch size: 300\")\\n\\nxlab = plt.xlabel(\"Iteration\")\\nylab = plt.ylabel(\"Score\")\\nplt.legend()\\n'\n\n\n\n\n\n\n\nConclusion to Experiment 2\nThe greater the size of each batch, the longer it takes for the algorithm to converge."
  },
  {
    "objectID": "posts/MLProject/Code.html",
    "href": "posts/MLProject/Code.html",
    "title": "Human Activity Recognition (HAR) - Final Project",
    "section": "",
    "text": "Literature Review:\nDiverging from the usage of HAR, a big and current topic is how to implement HAR. Since 2006, researchers and computer scientists have implemented different algorithms to determine the best algorithm to use. In the article titled, High Accuracy Human Activity Recognition Using Machine Learning and Wearable Devices’ Raw Signals, the authors explained the history of HAR research and the different algorithms used. First in 2006, Pirttijangas et al. “tested a model that used several multilayer perceptrons and k-nearest neighbors algorithms to recognize 17 activities to achieve an overall accuracy of 90.61%” (Papaleonidas, Psathas, and Iliadis). In 2011, Casale et al. “used a wearable device and applied a random forest classification algorithm to model five distinct activities (walking, climbing stairs, talking to a person, standing, and working on the computer)”, which achieved a 90% accuracy (Papaleonidas, Psathas, and Iliadis). In 2018, Brophy et al. “proposed a hybrid convolutional neural network and an SVM model with an accuracy of 92.3% for four activities (walking and running on a treadmill, low and high resistance bike exercise)” (Papaleonidas, Psathas, and Iliadis).\n\n\nImplementation\nThere are many ways and many algorithms that people have attempted to implement to find the best and most accurate model for Human Activity Recognition.\nLooking at these different implementations of Human Activity Recognition, we wanted to implement three algorithms, the k-nearest neighbor algorithm, the multilayer perceptron (also known as a fully connected neural network), and a random forest classifier, to determine the best accuracy given our data set. More about our data set can be found below.\n\nfrom os import listdir\nfrom pandas import read_csv\nimport pandas as pd\nimport numpy as np\nimport glob\nimport random\nfrom pandas import DataFrame\nfrom matplotlib import pyplot\nfrom numpy import vstack\nfrom numpy import unique\nfrom sklearn.metrics import accuracy_score\n\nFirst, let’s load in our data set using the three functions below:\n\n# load a single file as a numpy array\ndef load_file(filepath):\n    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n    return dataframe.values\n\n# load a group of files, such as x, y, z data for a given variable and  return them as a 3d numpy array\ndef load_group(filenames, prefix=''):\n    loaded = list()\n    for name in filenames:\n        data = load_file(prefix + name)\n        loaded.append(data)\n    # stack group so that features are the 3rd dimension\n    loaded = np.dstack(loaded)\n    return loaded\n\n# load a dataset type, such as train or test\ndef load_dataset(type, prefix=''):\n    path = prefix + type + '/Inertial Signals/'\n    \n    filenames = list()\n    # total acceleration\n    filenames += ['total_acc_x_' + type + '.txt', 'total_acc_y_' + type + '.txt', 'total_acc_z_' + type + '.txt']\n    # body acceleration\n    filenames += ['body_acc_x_' + type + '.txt', 'body_acc_y_' + type + '.txt', 'body_acc_z_' + type + '.txt']\n    # body gyroscope\n    filenames += ['body_gyro_x_' + type + '.txt', 'body_gyro_y_' + type + '.txt', 'body_gyro_z_' + type + '.txt']\n\n    # load input data\n    X = load_group(filenames, path)\n    # load output data\n    y = load_file(prefix + type + '/y_'+type+'.txt')\n    return X, y\n\nAs we load in the dataset, here is a description of the dataset as found in the ReadMe of the dataset:\n“The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix ‘t’ to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the ‘f’ to indicate frequency domain signals). These signals were used to estimate variables of the feature vector for each pattern: ‘-XYZ’ is used to denote 3-axial signals in the X, Y and Z directions” (Reyes-Ortiz et al.)\nNow let’s take a closer look at our data:\n\n# summarize the balance of classes in an output variable column\ndef class_breakdown(data):\n    # convert the numpy array into a dataframe\n    df = DataFrame(data)\n    # group data by the class value and calculate the number of rows\n    counts = df.groupby(0).size()\n    # retrieve raw rows\n    counts = counts.values\n    # summarize\n    for i in range(len(counts)):\n        percent = counts[i] / len(df) * 100\n        print('Class=%d, total=%d, percentage=%.3f' % (i+1, counts[i], percent))\n\n\n# load all train data\nprint('Training data')\ntrainX, trainy = load_dataset('train', 'UCI HAR Dataset/')\nclass_breakdown(trainy)\n# load all test data\nprint('Testing data')\ntestX, testy = load_dataset('test', 'UCI HAR Dataset/')\nclass_breakdown(testy)\n\nprint('Both')\ncombined = vstack((trainy, testy))\nclass_breakdown(combined)\n\nTraining data\nClass=1, total=1226, percentage=16.676\nClass=2, total=1073, percentage=14.595\nClass=3, total=986, percentage=13.411\nClass=4, total=1286, percentage=17.492\nClass=5, total=1374, percentage=18.689\nClass=6, total=1407, percentage=19.138\nTesting data\nClass=1, total=496, percentage=16.831\nClass=2, total=471, percentage=15.982\nClass=3, total=420, percentage=14.252\nClass=4, total=491, percentage=16.661\nClass=5, total=532, percentage=18.052\nClass=6, total=537, percentage=18.222\nBoth\nClass=1, total=1722, percentage=16.720\nClass=2, total=1544, percentage=14.992\nClass=3, total=1406, percentage=13.652\nClass=4, total=1777, percentage=17.254\nClass=5, total=1906, percentage=18.507\nClass=6, total=1944, percentage=18.876\n\n\nWe can see above that our data is equally split among the six activities. This is a good sign and tells us there is an equal distribution among the activities.\n\n# load data\nsub_map = load_file('UCI HAR Dataset/train/subject_train.txt')\ntrain_subjects = unique(sub_map)\nprint(\"The train subjects are:\", train_subjects)\n\nThe train subjects are: [ 1  3  5  6  7  8 11 14 15 16 17 19 21 22 23 25 26 27 28 29 30]\n\n\n\n# get all data for one subject\ndef data_for_subject(X, y, sub_map, sub_id):\n    # get row indexes for the subject id\n    ix = [i for i in range(len(sub_map)) if sub_map[i]==sub_id]\n    # return the selected samples\n    return X[ix, :, :], y[ix]\n\n\n# convert a series of windows to a 1D list\ndef to_series(windows):\n    series = list()\n    for window in windows:\n        # remove the overlap from the window\n        half = int(len(window) / 2) - 1\n        for value in window[-half:]:\n            series.append(value)\n    return series\n\nNow that we have fully loaded in the data and updated the data, we should take a look at what the data looks like. Below, we will plot two distinct graphs. The first graph will consist of two similar graphs from two different subjects. It will show the total acceleration, the body acceleration, and the body gyroscope for each activity over time.\n\ndef plot_subject(X, y, person):\n    pyplot.figure()\n    n, off = X.shape[2] + 1, 0\n    pyplot.suptitle('Person ' + str(person), y = 0.93)\n\n    # total acceleration\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('total acc ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    # body acceleration\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('body acc ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    # body gyro\n    for i in range(3):\n        pyplot.subplot(n, 1, off + 1)\n        pyplot.plot(to_series(X[:, :, off]))\n        pyplot.title('body gyro ' + str(i), y = 0, loc = 'left')\n        off += 1\n    \n    pyplot.subplot(n, 1, n)\n    pyplot.plot(y)\n    pyplot.title('activity', y=0, loc='left')\n    pyplot.show()\n\n\n# get the data for one subject\nperson = random.randint(0, len(train_subjects) - 1)\nsub_id = train_subjects[person]\nsubX, suby = data_for_subject(trainX, trainy, sub_map, sub_id)\n# plot data for subject\nplot_subject(subX, suby, person)\n\n\n\n\n\n# get the data for one subject\nperson2 = random.randint(0, len(train_subjects) - 1)\nif person2 == person:\n    person2 = random.randint(0, len(train_subjects) - 1)\nsub_id = train_subjects[person2]\nsubX, suby = data_for_subject(trainX, trainy, sub_map, sub_id)\n# plot data for subject\nplot_subject(subX, suby, person2)\n\n\n\n\nWithin the plots above we see different levels of activity. For the walking activities (walking, walking upstairs, and walking downstairs), which are associated with activities 1, 2, and 3, we see a greater amount of movement compared to activities 4, 5, and 6, which correspond to sitting, standing, and laying. When we see “stagnant” lines, we can correspond that to activities 4, 5, and 6.\nIn addition, for both people, the accelerations look very similar as they are completing the same tasks.\n\n# group data by activity\ndef data_by_activity(X, y, activities):\n    # create a dictionary where the activity is the key, and the subset of X is the value\n    result = {}\n    for a in activities:\n        mask = y[:, 0] == a\n        subset_X = X[mask, :, :]\n        result[a] = subset_X\n    return result\n\n\ndef plot_activity_histograms(X, y):\n    # get a list of unique activities for the subject\n    activity_ids = unique(y[:,0])\n    grouped = data_by_activity(X, y, activity_ids)\n    \n    # plot histograms per activity\n    pyplot.figure()\n    pyplot.suptitle('Person ' + str(person) + \"'s Total Acceleration for the Different Activities\", y = 0.93)\n    xaxis = None\n    for k in range(len(activity_ids)):\n        activ_id = activity_ids[k]\n\n        for i in range(3):\n            ax = pyplot.subplot(len(activity_ids), 1, k+1, sharex=xaxis)\n            ax.set_xlim(-1,1)\n            if k == 0:\n                xaxis = ax\n            # create histogram subplot by indexing the grouped activities dictionary by activity and total acceleration axis\n            pyplot.hist(to_series(grouped[activ_id][:,:,i]), bins=100)\n            pyplot.title('activity '+str(activ_id), y=0, loc='left')\n    pyplot.show()\n\n\nplot_activity_histograms(subX, suby)\n\nC:\\Users\\meadb\\AppData\\Local\\Temp\\ipykernel_10092\\80887460.py:25: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax = pyplot.subplot(len(activity_ids), 1, k+1, sharex=xaxis)\n\n\n\n\n\nTaking a look at the histogram above, we can see the total acceleration for each activity. We can see how some activities look vastly different from each other. For instance, activity 6 (laying down) is extremely different than activity 1 (walking).\nIt is also important to not the three colors, which correspond to the three axis. The x-axis is blue, the y-axis is orange, and the z-axis is green.\nNow that we have an understanding of what our data looks like, it is time to run some algorithms. We ran three algorithms on our data. The first algorithm used was the K-nearest neighbors algorithm. The second algorithm is a fully connected neural network. And the third and last algorithm is a random forest classifier. The three algorithms are imported from sklearn.\nThe three algorithms are seen below:\n\n# k Nearest Neighbors algorithm (imported from sklearn)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainy, test_size=0.33, random_state=42)\n\n#Average accuracy of knn for 10 models\n\nknn_models = []\n\nfor i in range(1,10):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train.reshape(X_train.shape[0], -1), y_train.ravel())\n    y_pred = knn.predict(X_test.reshape(X_test.shape[0], -1))\n    knn_models.append(accuracy_score(y_test, y_pred))\nprint(\"Highest Accuracy for KNN of Different Sizes:\", max(knn_models), \"for k =\", knn_models.index(max(knn_models))+1)\n\nHighest Accuracy for KNN of Different Sizes: 0.8978162340337865 for k = 1\n\n\n\nExplanation of the KNN algorithm:\nWe implemented KNN on our data set. We started by splitting the data into train and test data. Then we trained the KNN on the training data (we had to reshape the training data) with 1 neighbors. We tested out a few different values for n_neighbors, and we found that k = 1 gives us the best results. Finally, we predicted on testing data and got the accuracy score.\n\n# fully connected neural network (multilayer perceptron)\nfrom sklearn.neural_network import MLPClassifier\n\n#Average accuracy of neural networks for 10 models\nmlp_models = []\nn_features = trainX.shape[1] * trainX.shape[2]\ntrainX_mlp = trainX.reshape((trainX.shape[0], n_features))\ntestX_mlp = testX.reshape((testX.shape[0], n_features))\n\nfor i in range(10):\n    model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500)\n    model.fit(trainX_mlp, trainy.ravel())\n    mlp_models.append(model.score(testX_mlp, testy))\nprint(\"Average Accuracy for Neural Networks \" + str(np.mean(mlp_models)))\n\nAverage Accuracy for Neural Networks 0.8510349507974212\n\n\n\n\nExplanation of the fully connected neural network:\nBecause of randomization in the neural network, we chose to run it 10 times and take the average accuracy. We first loaded in the dataset and then reshaped the train and test data so that if works with the model. We fit the data and found the accuracy score.\n\n# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Average accuracy of random forest classifier for 10 models\n\nrfc_models = []\n\nfor i in range(10):\n    rfc = RandomForestClassifier(n_estimators = 100)\n    rfc.fit(trainX.reshape(len(trainX), -1), trainy.ravel())\n    y_pred = rfc.predict(testX.reshape(len(testX), -1))\n    rfc_models.append(accuracy_score(testy, y_pred))\nprint(\"Average Accuracy for Random Forest Classifier: \"+ str(np.mean(rfc_models)))\n\nAverage Accuracy for Random Forest Classifier: 0.8484221241940958\n\n\n\n\nExplanation of the random forest classifier:\nSimilar to what we did previously to account for randomness, we ran the algorith 10 times and took the average accuracy. We implemented the random forest classifier using 100 trees. We had to reshape the training data before training the data using the algorithm. Then, we predicted the test data and got the accuracy score.\n\n\n\nResults\nFrom the three algorithms above, we can see that the accuracy scores are close in all three algorithms. The K Nearest Neighbors Algorithm produced an accuracy score of 89.8%, the fully connected neural network produced an accuracy score of 85.0%, and the Random Forest Classifier produced an accuracy score of 85%. Given this, KNN seems to be the most accurate algorithm that we have. The other two’s performance are similar to each other.\n\n\nProject Contributions\n\nKaylynn\n\nLiterature Review\nInitial implementation of each algorithm\n\n\n\nZayn\n\nFound our dataset\nVisualized historgrams and graphs\n\n\n\nMead\n\nFound average accuracy of each algorithm\n\n\n\n\nConclusion\nIt is important the note the biases associated with our data and model.\nMost would agree that the broad usage fo HAR is used to benefit people’s physical health. Most usage includes tracking exercise. However, sometimes these models can also be used in a negative way. In the article titled, Vision-based Human Activity Recognition: A Survey, the researchers found that HAR has been used for surveillance and security. When facial recognition isn’t present, human recognition can be used to target people based on things like posture and walking pattern (Beddiar, D.R., Nini, B., Sabokrou, M. et al.). This is obviously very controversial. As we have seen previous with COMPAS, these security measures often target innocent people and are racially biased.\nFurthermore, we found issues associated with our data. Our data only includes people between the ages of 19-48. This can be problematic because it excludes children, many teens, and the elderly, which could make our results skewed. It is also important to note that the data only contains 30 volunteers, which is a very small number and isn’t representative of the general population. While not necessarily problematic, it makes our results inaccurate.\nIf we had more time, we could vastly improve our project.\nFirst, we would first run more our data with more algorithms, including convolutional neural networks and a support vector machine.\nFurthermore, I would use other HAR data and run the same algorithms on that data. This would allow us to compare our results to other data sets and see if our results are consistent.\nFinally, it would be nice to split the data into window data as opposed to snapshot data and running the algorithms that way.\n\n\nPersonal Reflection (Mead Gyawu)\nThe main thing that I learned about is some of the situations in which someone would use different classifiers. For example, lets compare random forests to knn in this scenario. Determining when to use a random forest in this example is heavily dependent on the number of trees and features the forest is observing whereas knn is dependent on the number of neighbors selected. Now while it may be possible to increase the accuracy of our implementation of a random forest classifier by changing the list of features, The construction and use of the decision trees in random forest ultimately resulted in a larger runtime than our implementation or knn. Given the fact that understanding the theory and mathematical foundations of algorithms has been one of my main goals for the semester, understanding this difference in why knn proved to be a more ideal classifier than random forest is key to that.\nAs I move forward into both the work force as well as other scenarios in which I may need to implement a classifier or algorithm that handles large sets of data, this understanding of the algorithms in this project as well as others I may use will be vital.\n\n\nSources:\nGupta, N., Gupta, S.K., Pathak, R.K. et al. Human activity recognition in artificial intelligence framework: a narrative review. Artif Intell Rev 55, 4755–4808 (2022). https://doi.org/10.1007/s10462-021-10116-x\nAntonios Papaleonidas, Anastasios Panagiotis Psathas & Lazaros Iliadis (2022) High accuracy human activity recognition using machine learning and wearable devices’ raw signals, Journal of Information and Telecommunication, 6:3, 237-253, DOI: 10.1080/24751839.2021.1987706\nBeddiar, D.R., Nini, B., Sabokrou, M. et al. Vision-based human activity recognition: a survey. Multimed Tools Appl 79, 30509–30555 (2020). https://doi.org/10.1007/s11042-020-09004-3\nReyes-Ortiz, Jorge L, et al. “Human Activity Recognition Using Smartphones Dataset.” UCI Machine Learning Repository, Nov. 2013, archive.ics.uci.edu/ml/index.php."
  },
  {
    "objectID": "posts/Penguins/ClassifyingPenguins.html",
    "href": "posts/Penguins/ClassifyingPenguins.html",
    "title": "Classifying Penguins",
    "section": "",
    "text": "Finding the 3 best features for a model to classify 3 different species of penguins.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\n\n\nThe model that I chose to work with is a Decision Tree Classifier. Here is how it performs without any method of feature selection:\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ntree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\ny_pred = tree.predict(X_test)\n\nprint(\"Accuracy of Decision Tree: \"+ str(accuracy_score(y_test,y_pred)))\n\nAccuracy of Decision Tree: 0.9852941176470589\n\n\nIn order to determine the 3 best feature, I employed recursive feature selection to get a ranking of the most impactful features on classification.\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import RFE\n\nsvc = SVC(kernel=\"linear\", C=1)\nrfe = RFE(estimator=svc, n_features_to_select=3)\nrfe.fit(X_train, y_train)\nranking = rfe.ranking_#.reshape(X_train.shape)\nprint(ranking)\n\n[ 3  2  8 11  6  1  1  5  7 12 10  9  4  1]\n\n\nThe Rankings go as follows:\n\nSex_MALE, Island_Biscoe\nCulmen Depth\nCulmen Length\n\nThe 3 features I chose were are Culmen Length (mm), Culmen Depth (mm), and Sex_MALE. Because Sex_MALE concerns gender, I included Sex_FEMALE as well.\n\nqual = ['Island_Biscoe',\n 'Island_Dream', 'Island_Torgersen', 'Stage_Adult, 1 Egg Stage',\n 'Clutch Completion_No', 'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE']\n\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncols = []\ni = 1\nwhile i < 4:\n    index = list(rfe.ranking_)[::-1].index(i)\n    cols.append(X_train.columns.values[::-1][index])\n    i += 1\n\n\nnew_cols = []\nfor i in cols:\n    if i in qual: new_cols.append(i)\n    if i in quant: new_cols.insert(0, i)\n\nif 'Sex_MALE' in new_cols: new_cols.append('Sex_FEMALE')\n\nprint(new_cols)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_MALE', 'Sex_FEMALE']\n\n\n\n\n\nHere is a table of the features chosen for my decision tree. The features in this table are grouped by Sex_MALE, and the other columns represent the average value of each of the other features chosen.\n\n#Table of supposed best columns\nprint(new_cols)\ntable = X_train.groupby('Sex_MALE').aggregate({\n    'Culmen Depth (mm)': \"mean\",\n    'Culmen Length (mm)' : \"mean\",\n    'Sex_FEMALE' : \"mean\"\n})\ntable\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_MALE', 'Sex_FEMALE']\n\n\n\n\n\n\n  \n    \n      \n      Culmen Depth (mm)\n      Culmen Length (mm)\n      Sex_FEMALE\n    \n    \n      Sex_MALE\n      \n      \n      \n    \n  \n  \n    \n      0\n      16.509524\n      42.051587\n      1.0\n    \n    \n      1\n      17.745385\n      46.364615\n      0.0\n    \n  \n\n\n\n\nHere is the accuracy of the decision tree now that we trained it on only these 3 features:\n\ntree = DecisionTreeClassifier()\ntree.fit(X_train[new_cols],y_train)\n\n\ny_pred = tree.predict(X_test[new_cols])\n\nprint(\"Accuracy of Decision Tree with these 3 features: \"+ str(accuracy_score(y_test,y_pred)))\n\nAccuracy of Decision Tree with these 3 features: 0.9705882352941176\n\n\nAs we can all see, there has been an increase in measured accuracy from the initial tree that we had to the tree trained on the selected features. In addition, if you look at the table, there is a strong correlation between larger measurements for Culmen length and depth with being Male. Considering that these are the 3 best features for classifying the penguins, it would make sense that there would be a strong correlation between these 3 features.\n\n\n\nHere is a visualization of the decision regions for the 3 species of penguins based on the features I have chosen.\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(tree, X_test[new_cols], y_test)"
  },
  {
    "objectID": "posts/Perceptron/Implementing_perceptron.html",
    "href": "posts/Perceptron/Implementing_perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "Plotting A Set of Points\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom Perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-2, -2), (2, 2)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nExperiments\n\n\n\n\nExperiment 1\nData is linearly separable then the perceptron algorithm converges to weight vector w describing a separating line. As more predicted labels created by Perceptron matches the actual labels, then the accuracy score given to the labels increases, and reaches a score of 1 (100%) when the predicted labels completely match the actual labels. Using my implementation of Perceptron, here is how the algorithm works on linearly separable data.\n\nP = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP.fit(X_,y)\n#print(\"History of Accuracy \\n\" + str(P.history))\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n#fig = plt.plot(P.history)\n#xlab = plt.xlabel(\"Iteration\")\n#ylab = plt.ylabel(\"Accuracy\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(P.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nChange In Accuracy Score\n\nfig = plt.plot(P.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nExperiment 2\nFor 2d data, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.Here is how the perceptron algorithm would function on data that is not linearly separable.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nP_2 = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP_2.fit(X_,y)\n#print(\"History of Accuracy \\n\" + str(P_2.history))\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(P_2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nChange In Accuracy Score\n\nfig = plt.plot(P_2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nExperiment 3\nBecause the perceptron algorithm is not limited to 2 dimensions, I decided to run the perceptron algorithm on a data set with 10 features, judge the accuracy, and based on that determine if the data is linearly separable.\n\n\nn = 100\np_features = 10\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nP_3 = Perceptron()\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nP_3.fit(X_,y)\n#print(\"History of Accuracy \\n\" + str(P_3.history))\nfig = plt.plot(P_3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nGiven the fact that the accuracy never reaches a score of 1.0, this data must not be linearly separable.\n\n\nRuntime Complexity\nUpdating the Perceptron algorithm is dependent on this equation:\n\\(w^{t+1}\\) = \\(w^{t}\\) + 1(yi <\\(w^{t}\\),xi> < 0)yixi\nCalculating <\\(w^{t}\\),xi> requires you to find the dot product of w(vector of weights) and x(the feature matrix). As a result we have a runtime of O(wx)."
  },
  {
    "objectID": "posts/TimnitGebru/LearningFromTimnitGebru.html",
    "href": "posts/TimnitGebru/LearningFromTimnitGebru.html",
    "title": "Learning From Dr. Timnit Gebru",
    "section": "",
    "text": "Gebru’s Talk Regarding Computer Vision\nDuring her talk, Dr. Gebru made emphasized the various issues and dangers present in the rollout and use of of computer vision. Initiall, facial recogniton technology released by major tech companies were extremely accurate when guessing the race and gender of White men, but were almost random in identifying Black women. This is the result of datasets that consistented mainly of or focused purely on male eurocentric features.\nShe highlighted the fact that because bias and intentionally flawed computer vision technology is often used by police, either to search for those with warrants or to find and arrest protestors, often times people have and will continue to be arrested by the police wrongfully. Given the fact that Black and hispanic populations are already disproportionately targeted, this will essentially become another tool in their oppression.\nThis goes te same for tools made to identify those who are transgendered or gay. Facial recognition technology that would classify those as gay or transgendered would be dangerous tools in the hands of those who seek to persecute them. Essentially, computer vision has already become a tool that furthers the discrimination that marginalized communities face.\n\n\nQuestion for Dr. Gebru\n\nThe knowledge and impacts of highly flawed facial recognition tools is public knowledge. However, companies like Google and Amazon refuse to publically acknowledge the harm that their algorithims can cause. What incentive or reason would these businesses have for releasing largely flawed tools?\n\n\n\nReflection\nIn both the class discussion as well as her talk, Dr. Gebru touched on the dangers and greed that defined the pursuit and investment in AI and AGI by major tech companies. The dangers that AI technologies pose to marginalized groups, the exploitation of people both in the US and abroad, and the hoarding of resources from smaller businesses pursuing specialized AI technology are things that I resonate with. These are all things we see businesses do in other industries so their presence in AI research and products is not a surprise. The thing that did surprise me, however, were the ties to eugenics that Dr. Gebru explained. When I typically here of eugenics, I mostly thought of what Dr. Gebru defined as first wave eugenics, which is the desire to improve human stock by getting rid of undesirable traits or people. This idea of eugenics had largely classist, racist, ethnically prejudiced, and ableist overtones. However, I never thought of the potential intersection of eugenics and AI. While this push for AGI could be seen as having these same overtones, eugenics, to me at the very least, just seemed like a way for wealthy and predominantly White elites to use a false understanding of biology to ensure their growth and power while ridding themselves of having to associate with those that they see below them. That’s not to say that those same elites would not use AGI to strengthen their control and hegemony, but more so that it would be through various tools used to oppress other groups that they see as undesirable."
  }
]